from langchain_community.llms import OpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.mapreduce import MapReduceChain
from langchain.prompts import PromptTemplate
import textwrap
from langchain.docstore.document import Document
from flask import Flask, request, render_template, send_file
from database import create_db_connection, fetch_text_from_db


# Initialize the LLM with OpenAsI
llm = OpenAI(temperature=0)


def split_text(text, max_length=1000):
    text_splitter = CharacterTextSplitter()
    return text_splitter.split_text(text)

# Function to load the summarization chain (refine)
def load_summarize_chain(llm, 
                         chain_type="refine"):
    
    prompt_template = """Write a concise summary of the following extracting the key information:\n\n{text}\n\nCONCISE SUMMARY:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["text"])

    refine_template = (
        "Your job is to produce a final summary\n"
        "We have provided an existing summary up to a certain point: {existing_answer}\n"
        "We have the opportunity to refine the existing summary"
        "(only if needed) with some more context below.\n"
        "------------\n{text}\n------------\n"
        "Given the new context, refine the original summary."
        "If the context isn't useful, return the original summary."
    )
    refine_prompt = PromptTemplate(input_variables=["existing_answer", "text"], template=refine_template)

    return MapReduceChain(llm, PROMPT, 
                          refine_prompt, 
                          return_intermediate_steps=True)

# def getText(document_id):
#         data = request.json
#     # conn = create_db_connection()
#     # if conn:
#     #     text = fetch_text_from_db(conn, document_id)
#     #     conn.close()
#     #     return text
#     # else:
#     #     return None

def summarize_large_text(text):
    # Split the large text into manageable chunks
    texts = split_text(text)

    # Convert each chunk into a Document
    docs = [Document(page_content=t) for t in texts]

    # Run the summarization chain
    chain = load_summarize_chain(llm, 
                                 chain_type="refine")
    
    output_summary = chain.run({"input_documents": docs}, return_only_outputs=True)
    
    # Format and print the summary
    wrapped_text = textwrap.fill(output_summary['output_text'], width=100, break_long_words=False, replace_whitespace=False)
    print(wrapped_text)
    return wrapped_text

# Call the summarization function
text = Flask(__name__)
summarized_text = summarize_large_text(text)
